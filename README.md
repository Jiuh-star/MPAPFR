# MPAPFR: Model Poisoning Attack Targeting Both Fairness and Robustness in Personalized Federated Learning

The code for this project is continuously being updated. Please note that the associated paper is currently under review, and the content of this repository may be subject to changes based on the ongoing review process.

We appreciate your understanding and encourage you to check back periodically for the latest updates.

---

## Project Structure

```
./
├─ configs         <-- Dataset setting and other hyperparameters.
├─ scripts         <-- Some useful scripts for running the experiments.
└─ src
   ├─ attacks      <-- Implementations of both backdoor and poisoning attacks.
   ├─ flcore       <-- Abstract class for federated learning, and utility functions.
   ├─ system       <-- Concrete implementation of federated learning algorithms.
   ├─ datasets.py  <-- Define and generate the FL datasets.
   ├─ models.py    <-- Define the models.
   ├─ robusts.py   <-- Define the extended robust aggregation rule.
   └─ main.py      <-- Main entry of the experiments.
```

## Quick Start

1. Install [rye](https://rye-up.com/) (A python package solution).
2. Install the dependencies.
    ```bash
    rye sync
    ```
3. Generate the dataset. The details of configuration check [DataSets](#datasets) and [About the Configuration](#about-the-configuration).
    ```bash
    pfl gendata -c ${your_dataset_config.toml}
    ```
4. (Optional) Check your data distribution.
    ```bash
    pfl plot -d ${your_data_path.fl}
    ```
5. Run the system. The details of configuration check [Supported Algorithms](#supported-algorithms) and [About the Configuration](#about-the-configuration).
    ```bash
    pfl run -c ${your_system_config.toml}
    ```
6. (Optional) Check the results with TensorBoard in the `workdir`.
    ```bash
    tensorboard --logdir=${workdir}
    ```
7. (Optional) Visualize the personalized models of different clients.
    ```bash
    pfl vis -c ${your_system_config.toml}
    ```

## About the Configuration

The configuration file is in the [TOML](https://toml.io/) format. The `pfl run` command will automatically save the running/actual configuration file to the working directory, so you will not forget your configuration. Each configuration are dynamically loaded and parsed, and to understand how they work you need to check the source code (especially where the `proper_call()` appear). Here is a brief introduction of the configuration file:

```toml
seed=0  # The random seed
io_policy = "real"  # Change the behavior of flcore.utils.atomic_io, dumping/loading the object to/from disk/RAM. (real | virtual).
protocol="FedAvg"  # The name of the federated learning algorithm. It should be the same as the class name in the `system` module.
dataset="data/xxx.fl"  # The path of the dataset file, which is generated by the `pfl gendata` command.
max_epoch=1000  # The maximum number of the global epochs.
tensorboard=true  # Whether to use TensorBoard to visualize the results.
eval_interval=1  # The interval of the evaluation.
concurrent=1  # The number of the threads to run the clients' training/inference concurrently. 0 means not to use multi-threading.
devices=["cuda:0"]  # The devices to run the experiments.

[client]
# The configurations for the clients. Check the source code of `FedAvg` and your `protocol` for more details.

[attacker]
# The configurations for the attacker. Check the source code of the attacker for more details.

[server]
# The configurations for the server. Check the source code of the server for more details.
```

And here is a brief introduction of the dataset configuration file:

```toml
seed=0  # The random seed
output="data/xxx.fl"  # The path to save the dataset file.

[dataset]
source="cifar10"  # The source dataset. Check the source code of `datasets.py` for more details.
num_clients=100  # The number of the clients.
resize=[32, 32]  # The size of the image.
min_data=40  # The minimum number of the data in each client.
fractions=[0.6, 0.2, 0.2]  # The fractions of the training, validation and test data of each client.
alpha=1  # (Optional) The hyperparameter for the Dirichlet data distribution.
# p_degree=0.5 # (Optional) The hyperparameter for the p-degree data distribution.

[dataset.dataloader_args]
# The arguments for the PyTorch DataLoader. Check the source code of PyTorch for more details.
```

## Datasets

You can define your own datasets in the `datasets.py` file. The following are the built-in datasets.

- `cifar10`: The CIFAR-10 dataset.
- `cifar100`: The CIFAR-100 dataset.
- `fashionmnist`: The Fashion-MNIST dataset.
- `mnist`: The MNIST dataset.
- `femnist`: The FEMNIST dataset.

Here are some common options for the dataset configuration file:

- `seed`: The random seed for the dataset generation.
- `output`: The path to save the dataset file.
- `dataset.source`: The source dataset. i.e., the function name in the `datasets.py` file.
- `num_clients`: The number of the clients that participate in the system.
- `resize`: The resized size of the image.
- `min_data`: The minimum number of the data in each client.
- `fractions`: The fractions of the training, validation and test data of each client.
- `alpha`: The hyperparameter for the Dirichlet data distribution.
- `p_degree`: The hyperparameter for the p-degree data distribution.
- `dataloader_args`: The arguments for the PyTorch DataLoader. e.g., `batch_size`, `num_workers`, etc.

## Models

The built-in models are listed below, but you can define your own models in the `models.py` file.

- `CnnModel`: A simple 2-layer CNN model for image classification.
- `DnnModel`: A simple fully connected neural network for image classification.
- `ResNet18`: A ResNet-18 model for image classification.
- `ResNet18GN`: A ResNet-18 model with Group Normalization for image classification.
- `LstmModel`: A simple LSTM model for sequence classification.

For some specific federated learning algorithms, it is required to have a layer named `HEAD_NAME="classifier"` in the model.

## Supported Algorithms

Currently, the following federated learning algorithms are supported.

**classifical federated learning algorithms**:

- [`FedAvg`](https://arxiv.org/abs/1602.05629): Communication-Efficient Learning of Deep Networks from Decentralized Data. 
- [`FedProx`](https://arxiv.org/abs/1812.06127): Federated Optimization in Heterogeneous Networks.

**personalized federated learning algorithms**:

- [`pFedMe`](https://arxiv.org/abs/2006.08848): Personalized Federated Learning with Moreau Envelopes.
- [`FedBN`](https://arxiv.org/abs/2102.07623): FedBN: Federated Learning on Non-IID Features via Local Batch Normalization.
- [`FedRep`](https://arxiv.org/abs/2102.07078): Exploiting Shared Representations for Personalized Federated Learning.
- [`FedALA`](https://arxiv.org/abs/2212.01197): FedALA: Adaptive Local Aggregation for Personalized Federated Learning.
- [`FedSelect`](https://arxiv.org/abs/2306.13264): FedSelect: Personalized Federated Learning with Customized Selection of Parameters for Fine-Tuning.
- [`FedAS`](https://openaccess.thecvf.com/content/CVPR2024/html/Yang_FedAS_Bridging_Inconsistency_in_Personalized_Federated_Learning_CVPR_2024_paper.html): FedAS: Bridging Inconsistency in Personalized Federated Learning
- [`Ditto`](https://arxiv.org/abs/2012.04221): Ditto: Fair and Robust Federated Learning Through Personalization
- [`GPFL`](https://arxiv.org/abs/2308.10279): GPFL: Simultaneously Learning Global and Personalized Feature Information for Personalized Federated Learning
- `FedAvgFT` and `FedProxFT`

`FedAvgFT` and `FedProxFT` are the fine-tuning the global model before the client evaluation. They are commonly used as baselines in the paper regarding personalized federated learning.

See [TsingZ0](https://github.com/TsingZ0)'s work on [PFLlib](https://github.com/TsingZ0/PFLlib) for the classification of algorithms.

Besides, all the algorithms support the robust aggregation rules, including `Krum`, `TrimmedMean`, and so on. Check the module `flcore.robust` and `robusts` for the details.

## Scripts

1. `deploy.py`: The script deploys the source code to the remote server. This requires the environment variables `REMOTE_HOST`, `USERNAME`, `PASSWORD` and `TARGET` to be set. A convenient way is to create a file named `.deploy.env` in the root directory with the following content:
    ```env
    REMOTE_HOST=xxx.xxx.xxx.xxx
    USERNAME=xxx
    PASSWORD=xxx
    TARGET=path/to/your/remote/dir
    ```
    Then you can run the following command to deploy the source code to the remote server (check `pyproject.toml` for details):
    ```bash
    rye run deploy
    ```
2. `execute.sh`: It executes the experiments in a parallel way. It is recommended to debug first before running it. The `template.in.toml` stores the common configurations of all the experiments, while special configurations are specificed with option `overwrite`. Note that `OMP_NUM_THREADS` and `MKL_NUM_THREADS` are set to **4** to avoid the memory leak issue in my device. You should manually set them to the proper number in your device.
3. `leak.py`: It checks the memory leak of the PyTorch model. It is recommended to run it before the experiments to check whether the memory leak exists. If not, then you can set the `OMP_NUM_THREADS` and `MKL_NUM_THREADS` to the maximum in the `execute.sh` to get the best performance.


## Tips

> [!TIP]
> If you have a huge memory, you can try to mount a `tmpfs` to the `workdir`, or set the `workdir` to a `tmpfs` (e.g., `/dev/shm/`) to speed up the training process. This save the intermediate product of the federated learning system, but may cause the operating system freeze when RAM exhausted.
> Setting the `io_policy="virtual"` is faster and recommended, which skip the serializing and deserializing procedure. However, the intermediate product of the federated learning system can not be recovered and checked when the program failed.

> [!TIP]
> The log file `run.log`, raw evaluation results `val.jsonl`, raw test result `test.json`, the final configuration file `config.toml` and TensorBoard file will be saved in the `workdir`.

> [!TIP]
> Your can change the dependencies in `pyproject.toml` when your CUDA driver not fit our requirements. e.g., for those `CUDA~=11.*.*`, updating the corresponding items to:
> ```toml
> dependencies = [
>   "torch==2.3.0+cu118",
>   "torchaudio>=2.2.2",
>   "torchvision>=0.17.2",
> ]
>
> [[tool.rye.sources]]
> name = "pytorch"
> url = "https://download.pytorch.org/whl/cu118"
> ```

> [!TIP]
> There are some works on the personalized federated learning that may be helpful for you:
> - [PFLlib](https://github.com/TsingZ0/PFLlib)
> - [FL-bench](https://github.com/KarhouTam/FL-bench)

> [!WARNING]
> Currently PyTorch might has serious memory leak with multi-threading on old Linux. A Temporary solution is to set `MKL_NUM_THREADS` and `OMP_NUM_THREADS` to a proper number. For more information see [Memory leak in multi-thread inference](https://github.com/pytorch/pytorch/issues/64412#issue-985957074) and [Memory Leak in MKL OpenMP on AVX2 machine](https://github.com/pytorch/pytorch/issues/64535#issue-988692443).
